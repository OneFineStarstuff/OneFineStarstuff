{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO4p2azK3eZtxcGWGwghBzO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStardust/blob/main/_Advanced_One_Fine_Starstuff_V17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPXfN5NwCIhX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import logging\n",
        "import argparse\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "# Define an advanced CNN model\n",
        "class AdvancedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = nn.MaxPool2d(2)(x)\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = nn.MaxPool2d(2)(x)\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = nn.MaxPool2d(2)(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Data transformations with advanced augmentations\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Datasets and loaders\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model, criterion, optimizer, and scheduler\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AdvancedCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # With weight decay\n",
        "scheduler = optim.lr_scheduler.SequentialLR(optimizer,\n",
        "    schedulers=[\n",
        "        optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=5),  # Warm-up for 5 epochs\n",
        "        optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)  # Cosine annealing for the remaining epochs\n",
        "    ],\n",
        "    milestones=[5]\n",
        ")\n",
        "\n",
        "# Mixed-precision scaler for AMP training\n",
        "scaler = GradScaler()\n",
        "\n",
        "# TensorBoard for tracking\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# Training loop with early stopping and checkpoints\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=20, patience=5):\n",
        "    best_val_acc = 0.0\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed-precision training\n",
        "            with autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
        "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Log to TensorBoard\n",
        "        writer.add_scalar('Training Loss', epoch_loss, epoch)\n",
        "        writer.add_scalar('Validation Loss', val_loss, epoch)\n",
        "        writer.add_scalar('Validation Accuracy', val_acc, epoch)\n",
        "\n",
        "        scheduler.step()  # Adjust learning rate with scheduler\n",
        "\n",
        "        # Early stopping and model checkpoint\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            logging.info(\"Best model saved.\")\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                logging.info(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * correct / total\n",
        "    return val_loss / len(data_loader.dataset), val_acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Train and evaluate an advanced CNN on CIFAR-10.')\n",
        "    parser.add_argument('--num_epochs', type=int, default=20, help='Number of epochs to train (default: 20)')\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate (default: 0.001)')\n",
        "    parser.add_argument('--batch_size', type=int, default=64, help='Batch size (default: 64)')\n",
        "    parser.add_argument('--patience', type=int, default=5, help='Early stopping patience (default: 5)')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    logging.info(\"Starting training...\")\n",
        "    train(model, train_loader, test_loader, criterion, optimizer, scheduler, args.num_epochs, args.patience)\n",
        "    logging.info(\"Training completed. Best model saved as 'best_model.pth'.\")\n",
        "    writer.close()"
      ]
    }
  ]
}