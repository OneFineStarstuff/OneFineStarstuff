{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOjWDIbeYsMIDZcWh2z6lih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/Multi_modal_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "class UnifiedAGISystem(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, sensor_dim, hidden_dim):\n",
        "        super(UnifiedAGISystem, self).__init__()\n",
        "        self.text_embed = nn.EmbeddingBag(text_dim, hidden_dim)\n",
        "        self.image_embed = nn.Sequential(\n",
        "            nn.Conv2d(image_dim[0], hidden_dim, kernel_size=3, stride=1, padding=1),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.sensor_embed = nn.Linear(sensor_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim * 3, 10)  # Example output size (adjust as needed)\n",
        "\n",
        "    def forward(self, text, image, sensor_data):\n",
        "        text_features = self.text_embed(text)\n",
        "        image_features = self.image_embed(image)\n",
        "        sensor_features = self.sensor_embed(sensor_data)\n",
        "        combined_features = torch.cat((text_features, image_features, sensor_features), dim=1)\n",
        "        return self.fc(combined_features)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.text_data = torch.randint(0, 512, (100, 10))  # (num_samples, text_dim)\n",
        "        self.image_data = torch.randn(100, 3, 224, 224)  # (num_samples, channels, height, width)\n",
        "        self.sensor_data = torch.randn(100, 10)  # (num_samples, sensor_dim)\n",
        "        self.labels = torch.randint(0, 10, (100,))  # 10 classes for classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.text_data[idx], self.image_data[idx], self.sensor_data[idx], self.labels[idx]\n",
        "\n",
        "def train(model, train_loader, optimizer, scheduler, criterion, epochs, device):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for text, image, sensor_data, labels in train_loader:\n",
        "            text, image, sensor_data, labels = text.to(device), image.to(device), sensor_data.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text, image, sensor_data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        epochs = 10\n",
        "\n",
        "        # Initialize model\n",
        "        model = UnifiedAGISystem(text_dim=512, image_dim=(3, 224, 224), sensor_dim=10, hidden_dim=128)\n",
        "\n",
        "        # Create Dataset and DataLoader\n",
        "        dataset = CustomDataset()\n",
        "        train_loader = DataLoader(dataset, batch_size=8, shuffle=True)  # Ensuring shuffling\n",
        "\n",
        "        # Optimizer and scheduler setup\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "        scheduler = OneCycleLR(optimizer, max_lr=5e-5, steps_per_epoch=len(train_loader), epochs=epochs)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Set device to CUDA if available, otherwise CPU\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Train the model\n",
        "        train(model, train_loader, optimizer, scheduler, criterion, epochs, device)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "W33eMwdP-94O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBqbIVhVs6wo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from transformers import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.multiprocessing import spawn\n",
        "\n",
        "# Define the model architecture\n",
        "class UnifiedAGISystem(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, sensor_dim, hidden_dim):\n",
        "        super(UnifiedAGISystem, self).__init__()\n",
        "        self.text_embed = nn.EmbeddingBag(text_dim, hidden_dim)\n",
        "        self.image_embed = nn.Conv2d(image_dim[0], hidden_dim, kernel_size=3, stride=1, padding=1)\n",
        "        self.sensor_embed = nn.Linear(sensor_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim * 3, 10)  # Example output size (adjust as needed)\n",
        "\n",
        "    def forward(self, text, image, sensor_data):\n",
        "        text_features = self.text_embed(text)\n",
        "        image_features = self.image_embed(image).view(image.size(0), -1)\n",
        "        sensor_features = self.sensor_embed(sensor_data)\n",
        "        combined_features = torch.cat((text_features, image_features, sensor_features), dim=1)\n",
        "        return self.fc(combined_features)\n",
        "\n",
        "# Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # Create synthetic data for demo purposes\n",
        "        self.text_data = torch.randint(0, 512, (100, 10))  # (num_samples, text_dim)\n",
        "        self.image_data = torch.randn(100, 3, 224, 224)  # (num_samples, channels, height, width)\n",
        "        self.sensor_data = torch.randn(100, 10)  # (num_samples, sensor_dim)\n",
        "        self.labels = torch.randint(0, 10, (100,))  # 10 classes for classification\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.text_data[idx], self.image_data[idx], self.sensor_data[idx], self.labels[idx]\n",
        "\n",
        "# Function to set environment variables for distributed training\n",
        "def set_environment_variables(rank, world_size):\n",
        "    import os\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'  # or the IP address of the master node\n",
        "    os.environ['MASTER_PORT'] = '29500'      # Choose a free port\n",
        "    os.environ['RANK'] = str(rank)\n",
        "    os.environ['WORLD_SIZE'] = str(world_size)\n",
        "\n",
        "# Training function for DDP\n",
        "def train_ddp(rank, world_size, model, train_loader, optimizer, scheduler, criterion, epochs):\n",
        "    try:\n",
        "        # Initialize the distributed process group\n",
        "        dist.init_process_group(\n",
        "            backend='nccl',\n",
        "            world_size=world_size,\n",
        "            rank=rank,\n",
        "            init_method='env://'\n",
        "        )\n",
        "\n",
        "        # Set device\n",
        "        torch.cuda.set_device(rank)\n",
        "        model.to(rank)\n",
        "\n",
        "        # Convert to DistributedDataParallel\n",
        "        model = nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
        "\n",
        "        # Create DistributedSampler\n",
        "        train_sampler = DistributedSampler(train_loader.dataset, num_replicas=world_size, rank=rank)\n",
        "        train_loader.sampler = train_sampler\n",
        "\n",
        "        print(f\"Rank {rank}/{world_size} initialized.\")\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            train_sampler.set_epoch(epoch)\n",
        "            running_loss = 0.0\n",
        "            for text, image, sensor_data, labels in train_loader:\n",
        "                text, image, sensor_data, labels = text.to(rank), image.to(rank), sensor_data.to(rank), labels.to(rank)\n",
        "\n",
        "                # Zero gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(text, image, sensor_data)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update loss\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Step the scheduler\n",
        "            scheduler.step()\n",
        "\n",
        "            if rank == 0:  # Print loss only from rank 0\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in rank {rank}: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Cleanup\n",
        "        dist.destroy_process_group()\n",
        "        print(f\"Rank {rank} finished.\")\n",
        "\n",
        "# Main function to setup distributed training\n",
        "def main():\n",
        "    try:\n",
        "        world_size = 2  # Number of GPUs available\n",
        "        epochs = 10\n",
        "\n",
        "        # Initialize model\n",
        "        model = UnifiedAGISystem(text_dim=512, image_dim=(3, 224, 224), sensor_dim=10, hidden_dim=128)\n",
        "\n",
        "        # Create Dataset and DataLoader\n",
        "        dataset = CustomDataset()\n",
        "        train_loader = DataLoader(dataset, batch_size=8)\n",
        "\n",
        "        # Optimizer and scheduler setup\n",
        "        optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "        scheduler = OneCycleLR(optimizer, max_lr=5e-5, steps_per_epoch=len(train_loader), epochs=epochs)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Ensure the number of GPUs is sufficient\n",
        "        if torch.cuda.device_count() < world_size:\n",
        "            raise ValueError(f\"Not enough GPUs available. Expected {world_size} but found {torch.cuda.device_count()}.\")\n",
        "\n",
        "        # Spawn processes for distributed training\n",
        "        print(f\"Starting distributed training with {world_size} processes.\")\n",
        "        spawn(train_ddp, nprocs=world_size, args=(world_size, model, train_loader, optimizer, scheduler, criterion, epochs))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}