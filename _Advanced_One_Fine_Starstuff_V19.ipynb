{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPxpO16pwV1hru3u0+EYcHM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStardust/blob/main/_Advanced_One_Fine_Starstuff_V19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaViM8y8iwaC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import logging\n",
        "from torch.amp import GradScaler, autocast\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import argparse\n",
        "\n",
        "# Setup Logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "# Enhanced SE Block for Attention\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n",
        "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, channels, _, _ = x.size()\n",
        "        out = x.view(batch, channels, -1).mean(dim=2)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = torch.sigmoid(self.fc2(out)).view(batch, channels, 1, 1)\n",
        "        return x * out\n",
        "\n",
        "# Residual Block with Optional SE Attention\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, use_se=False):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.se_block = SEBlock(out_channels) if use_se else nn.Identity()\n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        ) if in_channels != out_channels or stride != 1 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.se_block(out)\n",
        "        out += self.shortcut(x)\n",
        "        return torch.relu(out)\n",
        "\n",
        "# Advanced CNN with Residual Blocks and SE Attention\n",
        "class AdvancedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedCNN, self).__init__()\n",
        "        self.layer1 = ResidualBlock(3, 32, use_se=True)\n",
        "        self.layer2 = ResidualBlock(32, 64, stride=2, use_se=True)\n",
        "        self.layer3 = ResidualBlock(64, 128, stride=2, use_se=True)\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Data Augmentations\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    rand_index = torch.randperm(x.size(0))\n",
        "    target_a = y\n",
        "    target_b = y[rand_index]\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
        "    x[:, :, bbx1:bbx2, bby1:bby2] = x[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "    return x, target_a, target_b, lam\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "# Transformations, Dataset, and Data Loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model Setup, Loss, Optimizer, and Scheduler\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AdvancedCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=20)\n",
        "scaler = GradScaler()\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# Training and Evaluation Functions\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=20, patience=5, use_aug='mixup'):\n",
        "    best_val_acc = 0.0\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            if use_aug == 'mixup':\n",
        "                images, labels_a, labels_b, lam = mixup_data(images, labels)\n",
        "                optimizer.zero_grad()\n",
        "                with autocast(device_type='cuda'):\n",
        "                    outputs = model(images)\n",
        "                    loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "            elif use_aug == 'cutmix':\n",
        "                images, labels_a, labels_b, lam = cutmix_data(images, labels)\n",
        "                optimizer.zero_grad()\n",
        "                with autocast(device_type='cuda'):\n",
        "                    outputs = model(images)\n",
        "                    loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "            else:\n",
        "                optimizer.zero_grad()\n",
        "                with autocast(device_type='cuda'):\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "        writer.add_scalar('Training Loss', epoch_loss, epoch)\n",
        "        writer.add_scalar('Validation Loss', val_loss, epoch)\n",
        "        writer.add_scalar('Validation Accuracy', val_acc, epoch)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            logging.info(\"Best model saved.\")\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                logging.info(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * correct / total\n",
        "    return val_loss / len(data_loader.dataset), val_acc\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Train and evaluate the Advanced CNN on CIFAR-10.')\n",
        "    parser.add_argument('--num_epochs', type=int, default=20, help='Number of epochs to train (default: 20)')\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate (default: 0.001)')\n",
        "    parser.add_argument('--batch_size', type=int, default=64, help='Batch size (default: 64)')\n",
        "    parser.add_argument('--patience', type=int, default=5, help='Early stopping patience (default: 5)')\n",
        "    parser.add_argument('--augmentation', type=str, default='mixup', help='Augmentation technique (mixup/cutmix) (default: mixup)')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    logging.info(\"Starting training...\")\n",
        "    train(model, train_loader, test_loader, criterion, optimizer, scheduler, args.num_epochs, args.patience, use_aug=args.augmentation)\n",
        "    logging.info(\"Training completed. Best model saved as 'best_model.pth'.\")\n",
        "    writer.close()"
      ]
    }
  ]
}