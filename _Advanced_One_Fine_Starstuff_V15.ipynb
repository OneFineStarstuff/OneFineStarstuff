{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNG5yDG7R5mpBco+rSULV14",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStardust/blob/main/_Advanced_One_Fine_Starstuff_V15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwHUnwUPCwwZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import logging\n",
        "import argparse\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "# Define an advanced CNN model\n",
        "class AdvancedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = nn.MaxPool2d(2)(x)\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = nn.MaxPool2d(2)(x)\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = nn.MaxPool2d(2)(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define Label Smoothing Loss\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        confidence = 1.0 - self.smoothing\n",
        "        logprobs = F.log_softmax(output, dim=-1)\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        return (confidence * nll_loss + self.smoothing * smooth_loss).mean()\n",
        "\n",
        "# Mixup data augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Setup transformations, dataset, and data loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Model, loss function, optimizer, and scheduler\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AdvancedCNN().to(device)\n",
        "criterion = LabelSmoothingLoss(smoothing=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define learning rate warm-up and cosine annealing scheduler\n",
        "num_warmup_epochs = 5\n",
        "warmup_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=num_warmup_epochs)\n",
        "cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20 - num_warmup_epochs)\n",
        "scheduler = optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[num_warmup_epochs])\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=20, patience=5):\n",
        "    best_val_acc = 0.0\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Apply mixup\n",
        "            images, labels_a, labels_b, lam = mixup_data(images, labels)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Calculate mixup loss\n",
        "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
        "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save the best model and implement early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            logging.info(\"Best model saved.\")\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                logging.info(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * correct / total\n",
        "    return val_loss / len(data_loader.dataset), val_acc\n",
        "\n",
        "# Argument parsing and execution\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Train and evaluate an advanced CNN on CIFAR-10.')\n",
        "    parser.add_argument('--num_epochs', type=int, default=20, help='Number of epochs to train (default: 20)')\n",
        "    parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate (default: 0.001)')\n",
        "    parser.add_argument('--batch_size', type=int, default=64, help='Batch size (default: 64)')\n",
        "    parser.add_argument('--patience', type=int, default=5, help='Early stopping patience (default: 5)')\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    logging.info(\"Starting training...\")\n",
        "    train(model, train_loader, test_loader, criterion, optimizer, scheduler, args.num_epochs, args.patience)\n",
        "    logging.info(\"Training completed. Best model saved as 'best_model.pth'.\")"
      ]
    }
  ]
}