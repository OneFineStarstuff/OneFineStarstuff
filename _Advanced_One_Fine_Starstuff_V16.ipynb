{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNOe2wAd1xO+EEEoVWXmc3U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStardust/blob/main/_Advanced_One_Fine_Starstuff_V16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import logging\n",
        "import argparse\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "# Define an advanced CNN model\n",
        "class AdvancedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdvancedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)  # Adjusted for CIFAR-10 input size\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = nn.MaxPool2d(2)(x)\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = nn.MaxPool2d(2)(x)\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = nn.MaxPool2d(2)(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define Label Smoothing Loss\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        confidence = 1.0 - self.smoothing\n",
        "        logprobs = nn.functional.log_softmax(output, dim=-1)\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        return (confidence * nll_loss + self.smoothing * smooth_loss).mean()\n",
        "\n",
        "# Mixup data augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "# Set up transformations, dataset, and data loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "def setup_data_loaders(data_dir: str = './data', batch_size: int = 64) -> (DataLoader, DataLoader):\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Model, loss function, optimizer, and scheduler\n",
        "def setup_model_device(criterion: nn.Module, learning_rate: float = 0.001) -> (nn.Module, torch.device, optim.Optimizer, GradScaler, torch.cuda.amp.GradScaler):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = AdvancedCNN().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # With weight decay\n",
        "    scaler = GradScaler() if torch.cuda.is_available() else None\n",
        "    return model, device, optimizer, scaler, scaler\n",
        "\n",
        "def setup_scheduler(optimizer: optim.Optimizer, num_warmup_epochs: int = 5, total_epochs: int = 20) -> optim.lr_scheduler:\n",
        "    warmup_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=num_warmup_epochs)\n",
        "    cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_epochs - num_warmup_epochs)\n",
        "    scheduler = optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[num_warmup_epochs])\n",
        "    return scheduler\n",
        "\n",
        "# Training loop with early stopping and checkpoints\n",
        "def train(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader,\n",
        "          criterion: nn.Module, optimizer: optim.Optimizer, scheduler: optim.lr_scheduler,\n",
        "          device: torch.device, scaler, num_epochs: int = 20, patience: int = 5):\n",
        "    best_val_acc = 0.0\n",
        "    epochs_without_improvement = 0\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Apply mixup\n",
        "            images, labels_a, labels_b, lam = mixup_data(images, labels)\n",
        "            if torch.cuda.is_available():\n",
        "                # Mixed-precision training\n",
        "                with autocast():\n",
        "                    outputs = model(images)\n",
        "                    loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "            else:\n",
        "                # Standard precision training\n",
        "                outputs = model(images)\n",
        "                loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "\n",
        "            if scaler is not None:\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
        "        logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Log to TensorBoard\n",
        "        writer.add_scalar('Training Loss', epoch_loss, epoch)\n",
        "        writer.add_scalar('Validation Loss', val_loss, epoch)\n",
        "        writer.add_scalar('Validation Accuracy', val_acc, epoch)\n",
        "\n",
        "        scheduler.step()  # Adjust learning rate with scheduler\n",
        "\n",
        "        # Early stopping and model checkpoint\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            logging.info(\"Best model saved.\")\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                logging.info(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model: nn.Module, data_loader: DataLoader, criterion: nn.Module, device: torch.device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * correct / total\n",
        "    return val_loss / len(data_loader.dataset), val_acc\n",
        "\n",
        "# Argument parsing and execution\n",
        "if __name__ == '__main__':\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        # Running in a notebook environment\n",
        "        args = argparse.Namespace(\n",
        "            num_epochs=20,\n",
        "            learning_rate=0.001,\n",
        "            batch_size=64,\n",
        "            patience=5\n",
        "        )\n",
        "    else:\n",
        "        parser = argparse.ArgumentParser(description='Train and evaluate an advanced CNN on CIFAR-10.')\n",
        "        parser.add_argument('--num_epochs', type=int, default=20, help='Number of epochs to train (default: 20)')\n",
        "        parser.add_argument('--learning_rate', type=float, default=0.001, help='Learning rate (default: 0.001)')\n",
        "        parser.add_argument('--batch_size', type=int, default=64, help='Batch size (default: 64)')\n",
        "        parser.add_argument('--patience', type=int, default=5, help='Early stopping patience (default: 5)')"
      ],
      "metadata": {
        "id": "m7mkj1NZHlcN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}