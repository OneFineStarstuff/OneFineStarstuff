{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMoY1Gi3PTxSeTublk+9Gbh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/_Advanced_Research_and_Observations_Step_8_Integrate_Domain_Specific_Computational_Tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1PrXg9dvSYZ"
      },
      "outputs": [],
      "source": [
        "pip install weasyprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from scipy.stats import bayes_mvs\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import multiprocessing as mp\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import logging\n",
        "from jinja2 import Environment, FileSystemLoader\n",
        "import os\n",
        "from weasyprint import HTML\n",
        "\n",
        "# 1. Data Collection Class\n",
        "class DataCollection:\n",
        "    def __init__(self, data_source: str):\n",
        "        self.data_source = data_source\n",
        "        self.data = None\n",
        "\n",
        "    def collect_data(self) -> np.ndarray:\n",
        "        self.data = np.random.normal(0, 1, 1000)\n",
        "        print(\"Data collected from source.\")\n",
        "        return self.data\n",
        "\n",
        "    def preprocess_data(self) -> np.ndarray:\n",
        "        self.data = (self.data - np.mean(self.data)) / np.std(self.data)\n",
        "        print(\"Data preprocessed.\")\n",
        "        return self.data\n",
        "\n",
        "# 2. Error Analysis Class\n",
        "class ErrorAnalysis:\n",
        "    @staticmethod\n",
        "    def calculate_standard_error(data: np.ndarray) -> float:\n",
        "        n = len(data)\n",
        "        standard_error = np.std(data) / np.sqrt(n)\n",
        "        print(f\"Standard Error: {standard_error}\")\n",
        "        return standard_error\n",
        "\n",
        "    @staticmethod\n",
        "    def confidence_interval(data: np.ndarray, confidence: float = 0.95) -> tuple:\n",
        "        mean = np.mean(data)\n",
        "        sem = stats.sem(data)\n",
        "        margin = sem * stats.t.ppf((1 + confidence) / 2., len(data) - 1)\n",
        "        interval = (mean - margin, mean + margin)\n",
        "        print(f\"Confidence Interval ({confidence*100}%): {interval}\")\n",
        "        return interval\n",
        "\n",
        "# 3. Model Validation Class\n",
        "class ModelValidation:\n",
        "    def __init__(self, model, X: np.ndarray, y: np.ndarray):\n",
        "        self.model = model\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    def validate_model(self) -> float:\n",
        "        self.model.fit(self.X_train, self.y_train)\n",
        "        predictions = self.model.predict(self.X_test)\n",
        "        mse = mean_squared_error(self.y_test, predictions)\n",
        "        print(f\"Model Validation - MSE: {mse}\")\n",
        "        return mse\n",
        "\n",
        "    def k_fold_validation(self, k: int = 5) -> None:\n",
        "        kf = KFold(n_splits=k)\n",
        "        mse_scores = []\n",
        "        for train_index, test_index in kf.split(self.X_train):\n",
        "            X_train_kf, X_test_kf = self.X_train[train_index], self.X_train[test_index]\n",
        "            y_train_kf, y_test_kf = self.y_train[train_index], self.y_train[test_index]\n",
        "            self.model.fit(X_train_kf, y_train_kf)\n",
        "            predictions = self.model.predict(X_test_kf)\n",
        "            mse = mean_squared_error(y_test_kf, predictions)\n",
        "            mse_scores.append(mse)\n",
        "        mean_mse = np.mean(mse_scores)\n",
        "        print(f\"{k}-Fold Cross-Validation Mean MSE: {mean_mse}\")\n",
        "\n",
        "# 4. Scalable Computing Class for Parallel Processing\n",
        "class ScalableComputing:\n",
        "    @staticmethod\n",
        "    def parallel_computation(func, data: list, num_processes: int = 4) -> list:\n",
        "        with mp.Pool(num_processes) as pool:\n",
        "            results = pool.map(func, data)\n",
        "        print(\"Parallel computation completed.\")\n",
        "        return results\n",
        "\n",
        "# 5. Statistical Analysis Class\n",
        "class StatisticalAnalysis:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def hypothesis_test(self, pop_mean, alpha=0.05):\n",
        "        t_stat, p_value = stats.ttest_1samp(self.data, pop_mean)\n",
        "        print(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "        if p_value < alpha:\n",
        "            print(\"Reject the null hypothesis\")\n",
        "        else:\n",
        "            print(\"Fail to reject the null hypothesis\")\n",
        "        return t_stat, p_value\n",
        "\n",
        "    def bayesian_inference(self):\n",
        "        mean_ci, var_ci, std_ci = bayes_mvs(self.data, alpha=0.95)\n",
        "        print(f\"Bayesian Mean CI: {mean_ci}\")\n",
        "        print(f\"Bayesian Variance CI: {var_ci}\")\n",
        "        return mean_ci, var_ci, std_ci\n",
        "\n",
        "    def monte_carlo_simulation(self, func, num_simulations=1000):\n",
        "        results = [func(self.data) for _ in range(num_simulations)]\n",
        "        mean_result = np.mean(results)\n",
        "        print(f\"Monte Carlo Simulation Mean Result: {mean_result}\")\n",
        "        return results\n",
        "\n",
        "# 6. Visualization Class\n",
        "class Visualization:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def plot_distribution(self):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(self.data, kde=True)\n",
        "        plt.title(\"Data Distribution with KDE\")\n",
        "        plt.xlabel(\"Value\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.savefig(\"distribution_plot.png\")\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confidence_intervals(self, ci):\n",
        "        lower, upper = ci  # ci should be a tuple or list containing two values\n",
        "        mean = np.mean(self.data)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(self.data, kde=True)\n",
        "        plt.axvline(mean, color='blue', linestyle='--', label='Mean')\n",
        "        plt.axvline(lower, color='red', linestyle='--', label='Lower CI')\n",
        "        plt.axvline(upper, color='green', linestyle='--', label='Upper CI')\n",
        "        plt.title(\"Confidence Intervals\")\n",
        "        plt.legend()\n",
        "        plt.savefig(\"confidence_interval_plot.png\")\n",
        "        plt.show()\n",
        "\n",
        "# 7. Logger Class\n",
        "class Logger:\n",
        "    def __init__(self, log_file='research_log.log'):\n",
        "        logging.basicConfig(filename=log_file, level=logging.INFO,\n",
        "                            format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "        self.logger = logging.getLogger()\n",
        "\n",
        "    def log(self, message):\n",
        "        self.logger.info(message)\n",
        "        print(f\"LOG: {message}\")\n",
        "\n",
        "# 8. Report Generation Class\n",
        "class ReportGenerator:\n",
        "    def __init__(self, output_dir=\"reports\"):\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    def generate_report(self, context, template_name=\"report_template.html\"):\n",
        "        base_path = os.getcwd()\n",
        "        template_path = os.path.join(base_path, \"templates\")\n",
        "        os.makedirs(template_path, exist_ok=True)\n",
        "\n",
        "        # Check if template exists, create a simple one if it does not\n",
        "        template_file = os.path.join(template_path, template_name)\n",
        "        if not os.path.exists(template_file):\n",
        "            with open(template_file, \"w\") as file:\n",
        "                file.write(\n",
        "                    \"\"\"<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>{{ title }}</title>\n",
        "    <style>\n",
        "        body {\n",
        "            font-family: Arial, sans-serif;\n",
        "            margin: 20px;\n",
        "        }\n",
        "        h1 {\n",
        "            color: #333;\n",
        "        }\n",
        "        h2 {\n",
        "            color: #555;\n",
        "        }\n",
        "        table {\n",
        "            width: 100%;\n",
        "            border-collapse: collapse;\n",
        "            margin: 20px 0;\n",
        "        }\n",
        "        table, th, td {\n",
        "            border: 1px solid #ccc;\n",
        "        }\n",
        "        th, td {\n",
        "            padding: 10px;\n",
        "            text-align: left;\n",
        "        }\n",
        "        .plot {\n",
        "            text-align: center;\n",
        "            margin: 20px 0;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>{{ title }}</h1>\n",
        "\n",
        "    <h2>Data Summary</h2>\n",
        "    <table>\n",
        "        <tr>\n",
        "            <th>Metric</th>\n",
        "            <th>Value</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>Standard Error</td>\n",
        "            <td>{{ data_summary.standard_error }}</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>Confidence Interval</td>\n",
        "            <td>{{ data_summary.confidence_interval }}</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>Mean Squared Error (MSE)</td>\n",
        "            <td>{{ data_summary.mse }}</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>T-statistic</td>\n",
        "            <td>{{ data_summary.t_stat }}</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>P-value</td>\n",
        "            <td>{{ data_summary.p_value }}</td>\n",
        "        </tr>\n",
        "    </table>\n",
        "\n",
        "    <h2>Plots</h2>\n",
        "    <div class=\"plot\">\n",
        "        <h3>Distribution Plot</h3>\n",
        "        <img src=\"{{ plots.distribution_plot }}\" alt=\"Distribution Plot\">\n",
        "    </div>\n",
        "    <div class=\"plot\">\n",
        "        <h3>Confidence Interval Plot</h3>\n",
        "        <img src=\"{{ plots.confidence_interval_plot }}\" alt=\"Confidence Interval Plot\">\n",
        "    </div>\n",
        "\n",
        "    <h2>Conclusion</h2>\n",
        "    <p>{{ conclusion }}</p>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "                )\n",
        "\n",
        "        env = Environment(loader=FileSystemLoader(template_path))\n",
        "        template = env.get_template(template_name)\n",
        "\n",
        "        html_content = template.render(context)\n",
        "        report_path = os.path.join(self.output_dir, \"research_report.html\")\n",
        "\n",
        "        with open(report_path, \"w\") as file:\n",
        "            file.write(html_content)\n",
        "\n",
        "        pdf_path = os.path.join(self.output_dir, \"research_report.pdf\")\n",
        "\n",
        "        # Using WeasyPrint to generate PDF\n",
        "        HTML(report_path).write_pdf(pdf_path)\n",
        "\n",
        "        print(\"Report generated successfully.\")\n",
        "        return report_path, pdf_path\n",
        "\n",
        "# 9. Hyperparameter Optimization Class\n",
        "class HyperparameterOptimizer:\n",
        "    def __init__(self, model, param_grid):\n",
        "        self.model = model\n",
        "        self.param_grid = param_grid\n",
        "\n",
        "    def perform_grid_search(self, X, y):\n",
        "        grid_search = GridSearchCV(self.model, self.param_grid, cv=5, scoring=\"neg_mean_squared_error\")\n",
        "        grid_search.fit(X, y)\n",
        "        print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "        print(f\"Best Score (MSE): {-grid_search.best_score_}\")\n",
        "        return grid_search.best_estimator_\n",
        "\n",
        "# 10. Neural Network Model Class (Example in PyTorch)\n",
        "class NeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NeuralNetworkModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# 11. Training and Evaluation of Deep Learning Model\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model, criterion, optimizer):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def train(self, train_loader, epochs=10):\n",
        "        for epoch in range(epochs):\n",
        "            for data, target in train_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(data)\n",
        "                loss = self.criterion(outputs, target)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        total, correct = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                outputs = self.model(data)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += target.size(0)\n",
        "                correct += (predicted == target).sum().item()\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    X = np.random.rand(1000, 5)  # Sample input data\n",
        "    y = X @ np.array([1.5, -2.0, 0.5, 3.0, 2.5]) + np.random.normal(0, 0.1, 1000)  # Simulated output with noise\n",
        "\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "    rf_model = RandomForestRegressor()\n",
        "    optimizer = HyperparameterOptimizer(rf_model, param_grid)\n",
        "    best_model = optimizer.perform_grid_search(X, y)\n",
        "\n",
        "    # Logger\n",
        "    logger = Logger()\n",
        "    logger.log(\"Starting pipeline.\")\n",
        "\n",
        "    # Data Collection\n",
        "    data_collector = DataCollection(data_source=\"sensor\")\n",
        "    data = data_collector.collect_data()\n",
        "    data = data_collector.preprocess_data()\n",
        "\n",
        "    # Error Analysis\n",
        "    error_analysis = ErrorAnalysis()\n",
        "    std_error = error_analysis.calculate_standard_error(data)\n",
        "    ci = error_analysis.confidence_interval(data)\n",
        "\n",
        "    # Statistical Analysis\n",
        "    stats_analysis = StatisticalAnalysis(data)\n",
        "    t_stat, p_value = stats_analysis.hypothesis_test(pop_mean=0)\n",
        "\n",
        "    # Model Validation\n",
        "    X = np.random.rand(1000, 1)\n",
        "    y = 3.5 * X.flatten() + np.random.normal(0, 0.1, 1000)\n",
        "    model = LinearRegression()\n",
        "    validator = ModelValidation(model, X, y)\n",
        "    mse = validator.validate_model()\n",
        "    logger.log(f\"Model validation completed with MSE: {mse}\")\n",
        "\n",
        "    # Hyperparameter Optimization Example\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "    rf_model = RandomForestRegressor()\n",
        "    optimizer = HyperparameterOptimizer(rf_model, param_grid)\n",
        "    best_model = optimizer.perform_grid_search(X, y)\n",
        "\n",
        "    # Visualization\n",
        "    vis = Visualization(data)\n",
        "    vis.plot_distribution()\n",
        "    vis.plot_confidence_intervals(ci)\n",
        "\n",
        "    # Report\n",
        "    context = {\n",
        "        \"title\": \"Research Report\",\n",
        "        \"data_summary\": {\n",
        "            \"standard_error\": std_error,\n",
        "            \"confidence_interval\": ci,\n",
        "            \"mse\": mse,\n",
        "            \"t_stat\": t_stat,\n",
        "            \"p_value\": p_value,\n",
        "        },\n",
        "        \"plots\": {\n",
        "            \"distribution_plot\": \"distribution_plot.png\",\n",
        "            \"confidence_interval_plot\": \"confidence_interval_plot.png\"\n",
        "        },\n",
        "        \"conclusion\": \"The pipeline was executed successfully.\",\n",
        "    }\n",
        "    report_gen = ReportGenerator()\n",
        "\n",
        "    # Capture paths from generate_report\n",
        "    report_path, pdf_path = report_gen.generate_report(context)\n",
        "\n",
        "    print(\"PDF generated using WeasyPrint.\")"
      ],
      "metadata": {
        "id": "dat1nZiCwK_p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}