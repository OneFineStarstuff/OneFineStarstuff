{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPwB1EAEVVXH5fx0Sz2Pt8k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OneFineStarstuff/OneFineStarstuff/blob/main/_Advanced_Research_and_Observations_Step_9_Distributed_Computing_and_Parallel_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas dask scikit-learn matplotlib seaborn torch jinja2 weasyprint"
      ],
      "metadata": {
        "id": "Ugl7j55DIspX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dask[dataframe]"
      ],
      "metadata": {
        "id": "znyOHmll1IM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import logging\n",
        "from jinja2 import Environment, FileSystemLoader\n",
        "from weasyprint import HTML\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"data_params\": {\"mean\": 0, \"std\": 1, \"size\": 1000},\n",
        "    \"nn_params\": {\n",
        "        \"input_size\": 5,\n",
        "        \"hidden_size\": 10,\n",
        "        \"output_size\": 1,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"learning_rate\": 0.001,\n",
        "    },\n",
        "    \"output_dir\": \"reports\",\n",
        "    \"log_file\": \"pipeline.log\",\n",
        "}\n",
        "\n",
        "# Logger Setup\n",
        "logging.basicConfig(filename=CONFIG[\"log_file\"], level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# 1. Data Generation and Preprocessing\n",
        "def generate_data():\n",
        "    logger.info(\"Generating synthetic data.\")\n",
        "    X = np.random.rand(1000, CONFIG[\"nn_params\"][\"input_size\"])\n",
        "    y = X @ np.array([1.5, -2.0, 0.5, 3.0, 2.5]) + np.random.normal(0, 0.1, 1000)\n",
        "    return X, y\n",
        "\n",
        "def preprocess_data(X, y):\n",
        "    logger.info(\"Splitting data into training and testing sets.\")\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Statistical Analysis\n",
        "class StatisticalAnalysis:\n",
        "    @staticmethod\n",
        "    def hypothesis_test(data, pop_mean=0, alpha=0.05):\n",
        "        t_stat, p_value = stats.ttest_1samp(data, pop_mean)\n",
        "        logger.info(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n",
        "        return t_stat, p_value\n",
        "\n",
        "    @staticmethod\n",
        "    def confidence_interval(data, confidence=0.95):\n",
        "        mean = np.mean(data)\n",
        "        sem = stats.sem(data)\n",
        "        margin = sem * stats.t.ppf((1 + confidence) / 2., len(data) - 1)\n",
        "        ci = (mean - margin, mean + margin)\n",
        "        logger.info(f\"Confidence Interval: {ci}\")\n",
        "        return ci\n",
        "\n",
        "# 3. Visualization\n",
        "class Visualization:\n",
        "    @staticmethod\n",
        "    def plot_distribution(data, filename=\"distribution_plot.png\"):\n",
        "        sns.histplot(data, kde=True)\n",
        "        plt.title(\"Data Distribution\")\n",
        "        plt.xlabel(\"Value\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "        logger.info(f\"Distribution plot saved as {filename}.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_confidence_intervals(data, ci, filename=\"confidence_interval_plot.png\"):\n",
        "        mean = np.mean(data)\n",
        "        lower, upper = ci\n",
        "        sns.histplot(data, kde=True)\n",
        "        plt.axvline(mean, color='blue', linestyle='--', label='Mean')\n",
        "        plt.axvline(lower, color='red', linestyle='--', label='Lower CI')\n",
        "        plt.axvline(upper, color='green', linestyle='--', label='Upper CI')\n",
        "        plt.title(\"Confidence Intervals\")\n",
        "        plt.legend()\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "        logger.info(f\"Confidence interval plot saved as {filename}.\")\n",
        "\n",
        "# 4. Hyperparameter Optimization\n",
        "class HyperparameterOptimizer:\n",
        "    @staticmethod\n",
        "    def perform_grid_search(X, y):\n",
        "        param_grid = {\n",
        "            \"n_estimators\": [100, 200],\n",
        "            \"max_depth\": [10, 20],\n",
        "            \"min_samples_split\": [2, 5],\n",
        "        }\n",
        "        model = RandomForestRegressor()\n",
        "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring=\"neg_mean_squared_error\")\n",
        "        grid_search.fit(X, y)\n",
        "        logger.info(f\"Best Params: {grid_search.best_params_}\")\n",
        "        return grid_search.best_estimator_\n",
        "\n",
        "# 5. Neural Network Model and Training\n",
        "class NeuralNetworkModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NeuralNetworkModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model, criterion, optimizer):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def train(self, train_loader, epochs):\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            for inputs, targets in train_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            logger.info(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss:.4f}\")\n",
        "\n",
        "# 6. Report Generation\n",
        "class ReportGenerator:\n",
        "    def __init__(self, output_dir):\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    def generate_report(self, context):\n",
        "        template = \"\"\"<!DOCTYPE html>\n",
        "<html>\n",
        "<head><title>{{ title }}</title></head>\n",
        "<body>\n",
        "    <h1>{{ title }}</h1>\n",
        "    <h2>Data Summary</h2>\n",
        "    <p>Mean: {{ data_mean }}</p>\n",
        "    <p>Confidence Interval: {{ confidence_interval }}</p>\n",
        "    <h2>Plots</h2>\n",
        "    <img src=\"{{ distribution_plot }}\" alt=\"Distribution Plot\">\n",
        "    <img src=\"{{ ci_plot }}\" alt=\"Confidence Interval Plot\">\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "        env = Environment(loader=FileSystemLoader(\".\"))\n",
        "        env.from_string(template).render(context)\n",
        "        report_path = os.path.join(self.output_dir, \"report.html\")\n",
        "        with open(report_path, \"w\") as f:\n",
        "            f.write(template)\n",
        "        HTML(report_path).write_pdf(os.path.join(self.output_dir, \"report.pdf\"))\n",
        "        logger.info(\"Report generated successfully.\")\n",
        "\n",
        "# Main Execution Pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Pipeline started.\")\n",
        "\n",
        "    # Data Preparation\n",
        "    X, y = generate_data()\n",
        "    X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
        "\n",
        "    # Statistical Analysis\n",
        "    analysis = StatisticalAnalysis()\n",
        "    ci = analysis.confidence_interval(y_train)\n",
        "\n",
        "    # Visualization\n",
        "    Visualization.plot_distribution(y_train)\n",
        "    Visualization.plot_confidence_intervals(y_train, ci)\n",
        "\n",
        "    # Hyperparameter Optimization\n",
        "    optimizer = HyperparameterOptimizer()\n",
        "    best_model = optimizer.perform_grid_search(X_train, y_train)\n",
        "\n",
        "    # Neural Network Training\n",
        "    nn_model = NeuralNetworkModel(CONFIG[\"nn_params\"][\"input_size\"], CONFIG[\"nn_params\"][\"hidden_size\"], CONFIG[\"nn_params\"][\"output_size\"])\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(nn_model.parameters(), lr=CONFIG[\"nn_params\"][\"learning_rate\"])\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32).view(-1, 1))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"nn_params\"][\"batch_size\"], shuffle=True)\n",
        "    trainer = ModelTrainer(nn_model, criterion, optimizer)\n",
        "    trainer.train(train_loader, CONFIG[\"nn_params\"][\"epochs\"])\n",
        "\n",
        "    # Report Generation\n",
        "    report_gen = ReportGenerator(CONFIG[\"output_dir\"])\n",
        "    context = {\n",
        "        \"title\": \"Pipeline Report\",\n",
        "        \"data_mean\": np.mean(y_train),\n",
        "        \"confidence_interval\": ci,\n",
        "        \"distribution_plot\": \"distribution_plot.png\",\n",
        "        \"ci_plot\": \"confidence_interval_plot.png\",\n",
        "    }\n",
        "    report_gen.generate_report(context)\n",
        "    logger.info(\"Pipeline completed successfully.\")"
      ],
      "metadata": {
        "id": "_muFvaUvoV5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}